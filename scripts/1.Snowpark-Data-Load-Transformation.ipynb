{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40e74aaa",
   "metadata": {},
   "source": [
    "# Customer Churn Analysis\n",
    "\n",
    "### Problem Statement: \n",
    "#### We are tasked to build an end to end machine learning pipeline using snowpark for customer churn prediction in a telecom company to identify users who are at high risk of churning\n",
    "#### To accomplish this, we need to build a model that can learn how to identify such users, demonstrating with Snowflake/Snowpark to build a Classifier to help us with this task.\n",
    "\n",
    "### In this notebook we will -\n",
    "1. Load the raw parquet dataset\n",
    "2. EDA - clean and transform to create a dataset for model training\n",
    "3. Model Training and Deployment\n",
    "\n",
    "### Prerequisites\n",
    "1. Familiarity with basic Python and SQL\n",
    "2. Familiarity with training ML models\n",
    "3. Familiarity with data science notebooks\n",
    "4. Snowflake Account\n",
    "\n",
    "### What we'll Learn\n",
    "1. How to import/export data between Client and Snowflake\n",
    "2. How to conduct data cleaning and transformation using Snowpark\n",
    "3. How to train a model with Snowpark ML model\n",
    "4. How to visualize the predicted results from the model using packages like seaborn, matplotlib, plotly, streamlit\n",
    "5. How to convert the python code into an interactive streamlit app and make predictions on production data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b06da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snowpark API libraries\n",
    "from snowflake.snowpark import *\n",
    "from snowflake.snowpark.types import *\n",
    "from snowflake.snowpark.session import Session\n",
    "from snowflake.snowpark import functions as F\n",
    "import snowflake.snowpark.types as T\n",
    "from snowflake.snowpark import version\n",
    "from snowflake.snowpark.functions import udf\n",
    "\n",
    "# Snowpark ML libraries\n",
    "from snowflake.ml.modeling.preprocessing import OneHotEncoder\n",
    "from snowflake.snowpark.functions import udf, col, lit, translate, is_null, iff\n",
    "\n",
    "# Python libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import json\n",
    "print(version.VERSION)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a06360",
   "metadata": {},
   "source": [
    "###  Establishing a connection to the Snowflake database using Snowpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11729bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to Snowflake\n",
    "with open(\"creds.json\", \"r\") as f:\n",
    "    snowflake_conn_prop = json.load(f)  \n",
    "session = Session.builder.configs(snowflake_conn_prop).create()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a36d1c4",
   "metadata": {},
   "source": [
    "### Let's configure our Snowpark Session and initialize - \n",
    "#### DATABASE, WAREHOUSE, and SCHEMA that we will use for the remainder of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24477372",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#session.close()\n",
    "#session = Session.builder.configs(snowflake_conn_prop).create()\n",
    "session.sql(\"use role accountadmin\").collect()\n",
    "session.sql(\"create database if not exists  {}\".format(snowflake_conn_prop['database'])).collect()\n",
    "session.sql(\"use database {}\".format(snowflake_conn_prop['database'])).collect()\n",
    "session.sql(\"create schema if not exists {}\".format(snowflake_conn_prop['schema'])).collect()\n",
    "session.sql(\"use schema {}\".format(snowflake_conn_prop['schema'])).collect()\n",
    "session.sql(\"create or replace warehouse {} with \\\n",
    "                WAREHOUSE_SIZE = XSMALL \\\n",
    "                AUTO_SUSPEND = 120 \\\n",
    "                AUTO_RESUME = TRUE\".format(snowflake_conn_prop['warehouse'])).collect()\n",
    "session.sql(\"use warehouse {}\".format(snowflake_conn_prop['warehouse']))\n",
    "print(session.sql('select current_warehouse(), current_database(), current_schema()').collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5248d9b5",
   "metadata": {},
   "source": [
    "### Infer file schema & Load Data into snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f4833c",
   "metadata": {},
   "outputs": [],
   "source": [
    "folderpath = '/<path-to-the-folder>/customer-churn-prediction/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4606797b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert csv file to parquet file at the same location\n",
    "df = pd.read_csv(folderpath + 'telco_data_set.csv', low_memory=False)\n",
    "df.to_parquet(folderpath + 'telco_data_set.parquet', engine=\"fastparquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd15bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"telco_data_set.parquet\"\n",
    "stagename = \"RAWTELCODATA\"\n",
    "tablename = \"RAW_TELCO_PARQUET_DATA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8845b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sql(\"CREATE OR REPLACE FILE FORMAT MY_PARQUET_FORMAT TYPE = PARQUET;\").collect()\n",
    "\n",
    "session.sql(f\"CREATE OR REPLACE \\\n",
    "            TABLE {tablename} USING TEMPLATE ( \\\n",
    "                SELECT ARRAY_AGG(OBJECT_CONSTRUCT(*)) \\\n",
    "                FROM \\\n",
    "                    TABLE( INFER_SCHEMA( \\\n",
    "                    LOCATION => '@{stagename}/{filename}', \\\n",
    "                    FILE_FORMAT => 'MY_PARQUET_FORMAT' \\\n",
    "                    ) \\\n",
    "                ) \\\n",
    "            );  \").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b616dcda",
   "metadata": {},
   "source": [
    "### For incremental load "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5590f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRaw = session.read.option(\"compression\",\"snappy\").parquet(f\"@{stagename}/{filename}\")\n",
    "dfRaw.copy_into_table(tablename,FORCE= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a91e17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_data = session.table(tablename)\n",
    "#dfR.toPandas()\n",
    "# Drop the column with the empty header\n",
    "df_data = df_data.drop('Unnamed: 0')\n",
    "# Overwrite the original table with the updated dataframe\n",
    "df_data.write.mode(\"overwrite\").save_as_table(tablename)\n",
    "df_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403c753a",
   "metadata": {},
   "source": [
    "### Add a New Column 'CUSTOMERID' to the Snowpark Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de82055c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have an existing Snowpark DataFrame 'df'\n",
    "# Define the number of rows in the existing DataFrame\n",
    "num_rows = df_data.count()\n",
    "print(num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a8c9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_data\n",
    "cols_df1 = df1.columns\n",
    "cols_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c60f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the new column to the existing DataFrame\n",
    "df1 = df1.withColumn(\"CUSTOMERID\", F.expr(\"CAST(FLOOR(ABS(RANDOM()) * 1000000 + 1) AS INT)\"))\n",
    "\n",
    "# Ensure 6 digits by using LPAD\n",
    "df1 = df1.withColumn(\"CUSTOMERID\", F.expr(\"LPAD(CAST(CUSTOMERID AS VARCHAR), 6, '0')\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d27d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "rawtelcotable = \"RAW_TELCO_ID_TABLE\"\n",
    "\n",
    "# Overwrite the original table with the updated dataframe\n",
    "df1.write.mode(\"overwrite\").save_as_table(rawtelcotable)\n",
    "# Retrieve the updated table and display its contents\n",
    "df_data_ID = session.table(rawtelcotable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecc8457",
   "metadata": {},
   "source": [
    "### Categorical and Numerical Columns in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2c8922",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['GENDER','SENIORCITIZEN','PARTNER','DEPENDENTS','PHONESERVICE','MULTIPLELINES',\n",
    "            'INTERNETSERVICE','ONLINESECURITY','ONLINEBACKUP','DEVICEPROTECTION','TECHSUPPORT','STREAMINGTV','STREAMINGMOVIES',\n",
    "            'CONTRACT','PAPERLESSBILLING','PAYMENTMETHOD','TENUREMONTHSBIN','MONTHLYCHARGESBIN','TOTALCHARGESBIN']\n",
    "num_cols = [\"MONTHLYCHARGES\", \"TOTALCHARGES\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475461c1",
   "metadata": {},
   "source": [
    "### Number of Records in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59e1ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_ID.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d77f379",
   "metadata": {},
   "source": [
    "### Duplicates Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cd20cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates_df_data_ID = df_data_ID.group_by('CUSTOMERID').agg(F.count(('CUSTOMERID'))).filter(F.col('COUNT(CUSTOMERID)') > 1)\n",
    "print('Number Duplicates:', duplicates_df_data_ID.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d9b0bb",
   "metadata": {},
   "source": [
    "### Use the **drop_duplicates** to remove duplicated rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa2cbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_ID = df_data_ID.drop_duplicates('CUSTOMERID')\n",
    "df_data_ID.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b379ccf1",
   "metadata": {},
   "source": [
    "### Simple Statistics\n",
    "Obtaining simple statistics per column - why are some statistics missing? \\\n",
    "Can you already identify problems in our data? \\\n",
    "For example count always return the number of non null records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba31e547",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_ID.describe().show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bf445f",
   "metadata": {},
   "source": [
    "### Missing Value Imputation\n",
    "The describe output show that if we have missing values in the dataset. \\\n",
    "We will use the fillna method to replace missing values in any columns, if they are many we can create a new category for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c4586a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_ID = df_data_ID.fillna(value='0.0', subset=['TOTALCHARGES'])\n",
    "df_data_ID.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a7198b",
   "metadata": {},
   "source": [
    "### Finding constant variables\n",
    "How many distinct values do we have per column?  \n",
    "**Hint:** Constant values are probably irrelevant  \n",
    "**Hint:** Variables with many different values can be problematic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ec09e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = []\n",
    "for column in df_data_ID.columns:\n",
    "    unique_values.append([column, df_data_ID.select(column).distinct().count()])\n",
    "pd.DataFrame(unique_values, columns=['COLUMN_NAME','NUM_UNIQUE_VALUES'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bd0039",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d54808c",
   "metadata": {},
   "source": [
    "### Encoding: Prepare the data for model training by encoding categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582ee0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "OHE = OneHotEncoder(\n",
    "    input_cols=cat_cols,\n",
    "    output_cols=cat_cols,\n",
    "    drop_input_cols=True,\n",
    "    drop=\"first\",\n",
    "    handle_unknown=\"ignore\",\n",
    ")\n",
    "\n",
    "load_data_ohe = OHE.fit(df_data_ID).transform(df_data_ID)\n",
    "load_data_ohe.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c6b4c7",
   "metadata": {},
   "source": [
    "## Remove any spaces or '()' characters from the column names and convert to UPPER case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d51efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns = [load_data_ohe[col].alias(col.replace(' (automatic)', '')) for col in load_data_ohe.columns]\n",
    "load_data_ohe = load_data_ohe.select(*new_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1385f7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all columns to upper case using alias and replace spaces with underscores\n",
    "new_columns = [load_data_ohe[col].alias(col.replace(' ', '_').upper()) for col in load_data_ohe.columns]\n",
    "load_data_ohe = load_data_ohe.select(*new_columns)\n",
    "\n",
    "# Show the updated DataFrame\n",
    "load_data_ohe.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564c6311",
   "metadata": {},
   "source": [
    "### Assign to another dataframe to create the final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd83d55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_ID = load_data_ohe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903b11cc",
   "metadata": {},
   "source": [
    "### Validating columns in each DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db8bf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of columns in each DataFrame\n",
    "l_dfR_ID = len(df_data_ID.columns)\n",
    "print(l_dfR_ID)\n",
    "# Check the number of columns in each DataFrame\n",
    "r_dfR_ID = df_data_ID.count()\n",
    "print(r_dfR_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ff1aae",
   "metadata": {},
   "source": [
    "### The Snowpark API provides programming language constructs for building SQL statements. \\\n",
    "### It's a new developer experience which enables us to build code in :-\n",
    "\n",
    "<b><li>  Language of our choice </li></b>\n",
    "<b><li> Tool of our choice and </li></b>\n",
    "<b><li> Lazy execution to prevent multiple network hops to server </li></b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8188c91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_ID.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e8f089",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_ID.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3270524d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDemographics = df_data_ID.select(col(\"CUSTOMERID\"),\n",
    "                              translate(col(\"GENDER_MALE\"),lit(\"NULL\"),lit(\"Male\")).alias(\"GENDER\"),\n",
    "                              col(\"SENIORCITIZEN_YES\").alias(\"SENIORCITIZEN_YES\"),\n",
    "                              col(\"PARTNER_YES\"),\n",
    "                              col(\"DEPENDENTS_YES\")          \n",
    "                             )\n",
    "\n",
    "\n",
    "dfDemographics.write.mode('overwrite').saveAsTable('DEMOGRAPHICS')\n",
    "dfDemographics.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b8b976",
   "metadata": {},
   "source": [
    "## We can run transformation on data using similar dataframe API constructs, for example -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49423699",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sql(\"DROP TABLE IF EXISTS dfServices;\").collect()\n",
    "\n",
    "dfServices = df_data_ID.select(col(\"CUSTOMERID\"),\n",
    "                       col(\"PHONESERVICE_YES\").name(\"PHONESERVICE_YES\"),\n",
    "                       col(\"MULTIPLELINES_NO_PHONE_SERVICE\").name(\"MULTIPLELINES_NO_PHONE_SERVICE\"),\n",
    "                       col(\"MULTIPLELINES_YES\").name(\"MULTIPLELINES_YES\"),\n",
    "                       col(\"INTERNETSERVICE_FIBER_OPTIC\").name(\"INTERNETSERVICE_FIBER_OPTIC\"),\n",
    "                       col(\"INTERNETSERVICE_NO\").name(\"INTERNETSERVICE_NO\"),\n",
    "                       col(\"ONLINESECURITY_NO_INTERNET_SERVICE\").name(\"ONLINESECURITY_NO_INTERNET_SERVICE\"),\n",
    "                       col(\"ONLINESECURITY_YES\").name(\"ONLINESECURITY_YES\"),\n",
    "                       col(\"ONLINEBACKUP_NO_INTERNET_SERVICE\").name(\"ONLINEBACKUP_NO_INTERNET_SERVICE\"),\n",
    "                       col(\"ONLINEBACKUP_YES\").name(\"ONLINEBACKUP_YES\"),\n",
    "                       col(\"DEVICEPROTECTION_NO_INTERNET_SERVICE\").name(\"DEVICEPROTECTION_NO_INTERNET_SERVICE\"),\n",
    "                       col(\"DEVICEPROTECTION_YES\").name(\"DEVICEPROTECTION_YES\"),\n",
    "                       col(\"TECHSUPPORT_NO_INTERNET_SERVICE\").name(\"TECHSUPPORT_NO_INTERNET_SERVICE\"),\n",
    "                       col(\"TECHSUPPORT_YES\").name(\"TECHSUPPORT_YES\"),\n",
    "                       col(\"STREAMINGTV_NO_INTERNET_SERVICE\").name(\"STREAMINGTV_NO_INTERNET_SERVICE\"),\n",
    "                       col(\"STREAMINGTV_YES\").name(\"STREAMINGTV_YES\"),\n",
    "                       col(\"STREAMINGMOVIES_NO_INTERNET_SERVICE\").name(\"STREAMINGMOVIES_NO_INTERNET_SERVICE\"),\n",
    "                       col(\"STREAMINGMOVIES_YES\").name(\"STREAMINGMOVIES_YES\"),\n",
    "                       col(\"CONTRACT_ONE_YEAR\").name(\"CONTRACT_ONE_YEAR\"),\n",
    "                       col(\"CONTRACT_TWO_YEAR\").name(\"CONTRACT_TWO_YEAR\"),\n",
    "                       col(\"PAPERLESSBILLING_YES\").name(\"PAPERLESSBILLING_YES\"),\n",
    "                       col(\"PAYMENTMETHOD_CREDIT_CARD\").name(\"PAYMENTMETHOD_CREDIT_CARD\"),\n",
    "                       col(\"PAYMENTMETHOD_ELECTRONIC_CHECK\").name(\"PAYMENTMETHOD_ELECTRONIC_CHECK\"),\n",
    "                       col(\"PAYMENTMETHOD_MAILED_CHECK\").name(\"PAYMENTMETHOD_MAILED_CHECK\"),\n",
    "                       col(\"MONTHLYCHARGESBIN_LOW\").name(\"MONTHLYCHARGESBIN_LOW\"),\n",
    "                       col(\"MONTHLYCHARGESBIN_MEDIUM\").name(\"MONTHLYCHARGESBIN_MEDIUM\"),\n",
    "                       col(\"TOTALCHARGESBIN_LOW\").name(\"TOTALCHARGESBIN_LOW\"),\n",
    "                       col(\"TOTALCHARGESBIN_MEDIUM\").name(\"TOTALCHARGESBIN_MEDIUM\"),\n",
    "                       col(\"TENUREMONTHS\").name(\"TENUREMONTHS\"),\n",
    "                       col(\"MONTHLYCHARGES\").name(\"MONTHLYCHARGES\"),\n",
    "                       col(\"TOTALCHARGES\").name(\"TOTALCHARGES\"),\n",
    "                       col(\"CHURNVALUE\").name(\"CHURNVALUE\") \n",
    ")       \n",
    "\n",
    "dfServices.write.mode('overwrite').saveAsTable('TRAIN_DATASET_BIN')\n",
    "dfServices.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d9d054",
   "metadata": {},
   "source": [
    "## Create the TABLE in SNOWFLAKE - TRAINING DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3cc82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sql(\"DROP TABLE IF EXISTS TRAIN_CHURN_DATASET_BIN;\").collect()\n",
    "\n",
    "dfServices.write.mode(\"overwrite\").save_as_table(\"TRAIN_CHURN_DATASET_BIN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd587f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfServices.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bb54b7",
   "metadata": {},
   "source": [
    "## Off to ~02 notebook for exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af045ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "6297ec0394cd0a9f8996d9ae65d297808cbf00f3dbbe86db7fa7a80d51cc9063"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
